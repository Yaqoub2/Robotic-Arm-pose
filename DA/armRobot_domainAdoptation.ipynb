{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOlo4Ctmtm4p"
   },
   "source": [
    "# Deep transfer learning tutorial\n",
    "This notebook contains two popular paradigms of transfer learning: **Finetune** and **Domain adaptation**.\n",
    "Since most of the codes are shared by them, we show how they work in just one single notebook.\n",
    "I think that transfer learning and domain adaptation are both easy, and there's no need to create some library or packages for this simple purpose, which only makes things difficult.\n",
    "The purpose of this note book is we **don't even need to install a library or package** to train a domain adaptation or finetune model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P9av5SNtm4r"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcjr5uA7tm4r"
   },
   "source": [
    "First of all, install `pytorch` and `torchvision`.\n",
    "Skip this step if you already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3rx2L6hSvGY",
    "outputId": "114dc2a4-da08-46dc-e286-d17593fcc418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (0.19.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./miniconda3/envs/ibex_file/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWlAlAeRtm4s"
   },
   "source": [
    "Then, prepare the dataset you need. Here, we wil use the classical **Office-31** dataset.\n",
    "We just need to download it, and then extract it.\n",
    "Skip this step if you already have the data on your disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UCJrNkMtm4s"
   },
   "outputs": [],
   "source": [
    "!wget https://transferlearningdrive.blob.core.windows.net/teamdrive/dataset/office31.zip\n",
    "!unzip office31.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKVY0mi6tm4t"
   },
   "source": [
    "To verify the dataset, we show its structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXG19LWrciwk",
    "outputId": "a7ce42b2-19e5-4b9f-b068-c943b02f8b15"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYbnAtOLdKt5",
    "outputId": "c996522c-8c68-4408-bbc9-f20f7a39ba70"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/armRobotDA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip armRobotDA_full.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: apt: command not found\n",
      "\u001b[01;34marmRobotDA_full\u001b[0m\n",
      "├── \u001b[01;34mchair\u001b[0m\n",
      "│   ├── \u001b[01;34mE Down max, warning-samples\u001b[0m\n",
      "│   ├── \u001b[01;34mE up max warning-samples\u001b[0m\n",
      "│   ├── \u001b[01;34minitial, Safe -samples\u001b[0m\n",
      "│   ├── \u001b[01;34mS down max, warning-samples\u001b[0m\n",
      "│   └── \u001b[01;34mS up max, warning-samples\u001b[0m\n",
      "└── \u001b[01;34mscreen\u001b[0m\n",
      "    ├── \u001b[01;34mE Down max, warning-samples\u001b[0m\n",
      "    ├── \u001b[01;34mE up max warning-samples\u001b[0m\n",
      "    ├── \u001b[01;34minitial, Safe -samples\u001b[0m\n",
      "    ├── \u001b[01;34mS down max, warning-samples\u001b[0m\n",
      "    └── \u001b[01;34mS up max, warning-samples\u001b[0m\n",
      "1 [error opening dir]\n",
      "\n",
      "12 directories\n"
     ]
    }
   ],
   "source": [
    "!apt install tree\n",
    "!tree armRobotDA_full -d 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyP-9VnQtm4t"
   },
   "source": [
    "## Some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6R098oQTS_TC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torchvision import models\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9Ut17hwUDq-"
   },
   "source": [
    "Set the dataset folder, batch size, number of classes, and domain name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j4e--IIRU68M"
   },
   "outputs": [],
   "source": [
    "data_folder = 'armRobotDA_full'\n",
    "batch_size = 32\n",
    "n_class = 5\n",
    "domain_src, domain_tar = 'chair', 'screen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA6e2YaPtm4u"
   },
   "source": [
    "## Data load\n",
    "Now, define a data loader function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6JtN_bK9VFcM"
   },
   "outputs": [],
   "source": [
    "def load_data(root_path, domain, batch_size, phase):\n",
    "    transform_dict = {\n",
    "        'src': transforms.Compose(\n",
    "        [transforms.RandomResizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "         ]),\n",
    "        'tar': transforms.Compose(\n",
    "        [#transforms.Resize(224),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "         ])}\n",
    "    data = datasets.ImageFolder(root=os.path.join(root_path, domain), transform=transform_dict[phase])\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=3)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHy09lD9tm4v"
   },
   "source": [
    "Load the data using the above function to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jf_Gw2HRVJM_",
    "outputId": "bc358224-36c3-467d-da24-09671bffa4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data number: 8286\n",
      "Target data number: 628\n"
     ]
    }
   ],
   "source": [
    "src_loader = load_data(data_folder, domain_src, batch_size, phase='src')\n",
    "tar_loader = load_data(data_folder, domain_tar, batch_size, phase='tar')\n",
    "print(f'Source data number: {len(src_loader.dataset)}')\n",
    "print(f'Target data number: {len(tar_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caKMn438tm4v"
   },
   "source": [
    "## Define the finetune model\n",
    "The model for finetune is based on ResNet-50 for its popularity. Of course you can use other base networks.\n",
    "The main logic of this class is to get the pretrained ResNet-50, use all of its layers but the last one, which we will replace by a new FC layer for classification. Since the original ResNet-50 is for 1000 classes classification, we only need it to classify 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OXAjmY7pVK8t"
   },
   "outputs": [],
   "source": [
    "class TransferModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                base_model : str = 'resnet50',\n",
    "                pretrain : bool = True,\n",
    "                n_class : int = 5):\n",
    "        super(TransferModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.pretrain = pretrain\n",
    "        self.n_class = n_class\n",
    "        if self.base_model == 'resnet50':\n",
    "            self.model = torchvision.models.resnet50(pretrained=True)\n",
    "            n_features = self.model.fc.in_features\n",
    "            fc = torch.nn.Linear(n_features, n_class)\n",
    "            self.model.fc = fc\n",
    "        else:\n",
    "            # Use other models you like, such as vgg or alexnet\n",
    "            pass\n",
    "        self.model.fc.weight.data.normal_(0, 0.005)\n",
    "        self.model.fc.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAqT-0jRtm4w"
   },
   "source": [
    "Now, we define a model and test it using a random tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "id": "LewRmYIvXEIo",
    "outputId": "a619c65a-51e2-44d4-fbe2-e95f658b26ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wedadyo/miniconda3/envs/ibex_file/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wedadyo/miniconda3/envs/ibex_file/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2167, 0.2968, 0.1471, 0.0718, 0.0988]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "model = TransferModel().cuda()\n",
    "RAND_TENSOR = torch.randn(1, 3, 224, 224).cuda()\n",
    "output = model(RAND_TENSOR)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpX-fuiXtm4w"
   },
   "source": [
    "## Finetune ResNet-50\n",
    "Define some variables. Note that Office-31 doesn't have a validation set, so we use its target domain as the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uNeAiPatm4w"
   },
   "source": [
    "Now the most important part: write the finetune function.\n",
    "This function is pretty easy: it is basically a standard classification function. We train it on the 'src' domain, valid it on the 'val' domain, and then test it on the 'tar' domain.\n",
    "The only difference is that Office-31 dataset has no validation set, so we will use the target domain as the validation set. For your own data, you should use its standard validation set.\n",
    "We also set an early_stop variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "h74gKIVqtm4w"
   },
   "outputs": [],
   "source": [
    "dataloaders = {'src': src_loader,\n",
    "               'val': tar_loader,\n",
    "               'tar': tar_loader}\n",
    "n_epoch = 100\n",
    "n_epoch_da = 40\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stop = 20\n",
    "early_stop_da = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-fz_FlAIXsTF"
   },
   "outputs": [],
   "source": [
    "def finetune(model, dataloaders, optimizer):\n",
    "    since = time.time()\n",
    "    best_acc = 0\n",
    "    stop = 0\n",
    "    for epoch in range(0, n_epoch):\n",
    "        stop += 1\n",
    "        # You can uncomment this line for scheduling learning rate\n",
    "        # lr_schedule(optimizer, epoch)\n",
    "        for phase in ['src', 'val', 'tar']:\n",
    "            if phase == 'src':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            total_loss, correct = 0, 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'src'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                preds = torch.max(outputs, 1)[1]\n",
    "                if phase == 'src':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                correct += torch.sum(preds == labels.data)\n",
    "            epoch_loss = total_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = correct.double() / len(dataloaders[phase].dataset)\n",
    "            print(f'Epoch: [{epoch:02d}/{n_epoch:02d}]---{phase}, loss: {epoch_loss:.6f}, acc: {epoch_acc:.4f}')\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                stop = 0\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), 'model.pkl')\n",
    "        if stop >= early_stop:\n",
    "            break\n",
    "        print()\n",
    "\n",
    "    time_pass = time.time() - since\n",
    "    print(f'Training complete in {time_pass // 60:.0f}m {time_pass % 60:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGUZGrm7tm4x"
   },
   "source": [
    "Now, define some train parameters and the optimizer. For simplicity, we use SGD, and the learning rate for the FC layer is 10 times of other layers, which is a common trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HGVIu0JXZaGT"
   },
   "outputs": [],
   "source": [
    "param_group = []\n",
    "learning_rate = 0.0001\n",
    "momentum = 5e-4\n",
    "for k, v in model.named_parameters():\n",
    "    if not k.__contains__('fc'):\n",
    "        param_group += [{'params': v, 'lr': learning_rate}]\n",
    "    else:\n",
    "        param_group += [{'params': v, 'lr': learning_rate * 10}]\n",
    "optimizer = torch.optim.AdamW(param_group)\n",
    "#,momentum=momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7zGR_PFtm4y"
   },
   "source": [
    "## Train and test\n",
    "Now we can train and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKKrah-AZsHt"
   },
   "outputs": [],
   "source": [
    "finetune(model, dataloaders, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GR5Y1x4btm4y"
   },
   "outputs": [],
   "source": [
    "def test(model, target_test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    len_target_dataset = len(target_test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data, target in target_test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            s_output = model.predict(data)\n",
    "            pred = torch.max(s_output, 1)[1]\n",
    "            correct += torch.sum(pred == target)\n",
    "    acc = correct.double() / len(target_test_loader.dataset)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1O2CBxmxtm4y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4119384/2129991207.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model.pkl'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6544585987261147\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pkl'))\n",
    "acc_test = test(model, dataloaders['tar'])\n",
    "print(f'Test accuracy: {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7Iuk_Mitm4z"
   },
   "source": [
    "It's over for finetune. Of course, you should use some learning rate decay trick in real training. But that is not our goal.\n",
    "Next, we will continue to use the same dataloader for domain adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO4c_QcGtm4z"
   },
   "source": [
    "## Domain adaptation\n",
    "Now we are in domain adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJwcwLQftm40"
   },
   "source": [
    "## Logic for domain adaptation\n",
    "The logic for domain adaptation is mostly similar to finetune, except that we must add a loss to the finetune model to **regularize the distribution discrepancy** between two domains.\n",
    "Therefore, the most different parts are:\n",
    "- Define some **loss function** to compute the distance (which is the main contribution of most existing DA papers)\n",
    "- Define a new model class to use that loss function for **forward** pass.\n",
    "- Write a slightly different script to train, since we have to take both **source data, source label, and target data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy_1xwdJtm40"
   },
   "source": [
    "### Loss function\n",
    "The most popular loss function for DA is **MMD (Maximum Mean Discrepancy)**. For comaprison, we also use another popular loss **CORAL (CORrelation ALignment)**. They are defined as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3-wKorUtm40"
   },
   "source": [
    "#### MMD loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MpQH6VFwtm41"
   },
   "outputs": [],
   "source": [
    "class MMD_loss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcfUy_2Dtm41"
   },
   "source": [
    "#### CORAL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uZhKJq15tm41"
   },
   "outputs": [],
   "source": [
    "def CORAL(source, target):\n",
    "    d = source.size(1)\n",
    "    ns, nt = source.size(0), target.size(0)\n",
    "\n",
    "    # source covariance\n",
    "    tmp_s = torch.ones((1, ns)).cuda() @ source\n",
    "    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n",
    "\n",
    "    # target covariance\n",
    "    tmp_t = torch.ones((1, nt)).cuda() @ target\n",
    "    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n",
    "\n",
    "    # frobenius norm\n",
    "    loss = (cs - ct).pow(2).sum().sqrt()\n",
    "    loss = loss / (4 * d * d)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB2cDp8Gtm41"
   },
   "source": [
    "### Model\n",
    "Now we use ResNet-50 again just like finetune. The difference is that we rewrite the ResNet-50 class to drop its last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UOLx_OSxtm41"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "class ResNet50Fc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Fc, self).__init__()\n",
    "        model_resnet50 = models.resnet50(pretrained=True)\n",
    "        self.conv1 = model_resnet50.conv1\n",
    "        self.bn1 = model_resnet50.bn1\n",
    "        self.relu = model_resnet50.relu\n",
    "        self.maxpool = model_resnet50.maxpool\n",
    "        self.layer1 = model_resnet50.layer1\n",
    "        self.layer2 = model_resnet50.layer2\n",
    "        self.layer3 = model_resnet50.layer3\n",
    "        self.layer4 = model_resnet50.layer4\n",
    "        self.avgpool = model_resnet50.avgpool\n",
    "        self.__in_features = model_resnet50.fc.in_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def output_num(self):\n",
    "        return self.__in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRgdNM3Wtm42"
   },
   "source": [
    "Now the main class for DA. We take ResNet-50 as its backbone, add a bottleneck layer and our own FC layer for classification.\n",
    "Note the `adapt_loss` function. It is just using our predefined MMD or CORAL loss. Of course you can use your own loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oC5NKJpJtm42"
   },
   "outputs": [],
   "source": [
    "class TransferNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_class,\n",
    "                 base_net='resnet50',\n",
    "                 transfer_loss='mmd',\n",
    "                 use_bottleneck=True,\n",
    "                 bottleneck_width=256,\n",
    "                 width=1024):\n",
    "        super(TransferNet, self).__init__()\n",
    "        if base_net == 'resnet50':\n",
    "            self.base_network = ResNet50Fc()\n",
    "        else:\n",
    "            # Your own basenet\n",
    "            return\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        self.transfer_loss = transfer_loss\n",
    "        bottleneck_list = [nn.Linear(self.base_network.output_num(\n",
    "        ), bottleneck_width), nn.BatchNorm1d(bottleneck_width), nn.ReLU(), nn.Dropout(0.5)]\n",
    "        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n",
    "        classifier_layer_list = [nn.Linear(self.base_network.output_num(), width), nn.ReLU(), nn.Dropout(0.5),\n",
    "                                 nn.Linear(width, num_class)]\n",
    "        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n",
    "\n",
    "        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n",
    "        self.bottleneck_layer[0].bias.data.fill_(0.1)\n",
    "        for i in range(2):\n",
    "            self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n",
    "            self.classifier_layer[i * 3].bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = self.base_network(source)\n",
    "        target = self.base_network(target)\n",
    "        source_clf = self.classifier_layer(source)\n",
    "        if self.use_bottleneck:\n",
    "            source = self.bottleneck_layer(source)\n",
    "            target = self.bottleneck_layer(target)\n",
    "        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n",
    "        return source_clf, transfer_loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        features = self.base_network(x)\n",
    "        clf = self.classifier_layer(features)\n",
    "        return clf\n",
    "\n",
    "    def adapt_loss(self, X, Y, adapt_loss):\n",
    "        \"\"\"Compute adaptation loss, currently we support mmd and coral\n",
    "\n",
    "        Arguments:\n",
    "            X {tensor} -- source matrix\n",
    "            Y {tensor} -- target matrix\n",
    "            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- adaptation loss tensor\n",
    "        \"\"\"\n",
    "        if adapt_loss == 'mmd':\n",
    "            mmd_loss = MMD_loss()\n",
    "            loss = mmd_loss(X, Y)\n",
    "        elif adapt_loss == 'coral':\n",
    "            loss = CORAL(X, Y)\n",
    "        else:\n",
    "            # Your own loss\n",
    "            loss = 0\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAdPs27btm42"
   },
   "source": [
    "### Train\n",
    "Now the train part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OK6P8uMDtm42"
   },
   "outputs": [],
   "source": [
    "transfer_loss = 'coral'\n",
    "learning_rate = 0.0001\n",
    "transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet50').cuda()\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': transfer_model.base_network.parameters()},\n",
    "    {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10* learning_rate},\n",
    "    {'params': transfer_model.classifier_layer.parameters(), 'lr': 10* learning_rate},\n",
    "], lr=learning_rate, weight_decay=1e-4)\n",
    "# momentum=0.9, , weight_decay=5e-4\n",
    "lamb = 10 # weight for transfer loss, it is a hyperparameter that needs to be tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WpUfcHItm42"
   },
   "source": [
    "The main train function. Since we have to enumerate all source and target samples, we have to use `zip` operation to enumerate each pair of these two domains. It is common that two domains have different sizes, but we think by randomly sampling them in many epochs, we may sample each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AGlVNI2ktm42"
   },
   "outputs": [],
   "source": [
    "def train(dataloaders, model, optimizer):\n",
    "    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n",
    "    len_source_loader = len(source_loader)\n",
    "    len_target_loader = len(target_train_loader)\n",
    "    best_acc = 0\n",
    "    stop = 0\n",
    "    n_batch = min(len_source_loader, len_target_loader)\n",
    "    for e in range(n_epoch):\n",
    "        stop += 1\n",
    "        train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n",
    "        model.train()\n",
    "        for (src, tar) in zip(source_loader, target_train_loader):\n",
    "            data_source, label_source = src\n",
    "            data_target, _ = tar\n",
    "            data_source, label_source = data_source.cuda(), label_source.cuda()\n",
    "            data_target = data_target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            label_source_pred, transfer_loss = model(data_source, data_target)\n",
    "            clf_loss = criterion(label_source_pred, label_source)\n",
    "            loss = clf_loss + lamb * transfer_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n",
    "            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n",
    "            train_loss_total = loss.detach().item() + train_loss_total\n",
    "        acc = test(model, target_test_loader)\n",
    "        print(f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n",
    "        if best_acc < acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'trans_model.pkl')\n",
    "            stop = 0\n",
    "        if stop >= early_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nqSCG6-Xtm43",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/100], cls_loss: 1.3798, transfer_loss: 0.0002, total_Loss: 1.3820, acc: 0.5159\n",
      "Epoch: [ 1/100], cls_loss: 0.5882, transfer_loss: 0.0002, total_Loss: 0.5900, acc: 0.6258\n",
      "Epoch: [ 2/100], cls_loss: 0.3892, transfer_loss: 0.0001, total_Loss: 0.3906, acc: 0.7436\n",
      "Epoch: [ 3/100], cls_loss: 0.2526, transfer_loss: 0.0001, total_Loss: 0.2539, acc: 0.6210\n",
      "Epoch: [ 4/100], cls_loss: 0.3372, transfer_loss: 0.0001, total_Loss: 0.3383, acc: 0.6736\n",
      "Epoch: [ 5/100], cls_loss: 0.2976, transfer_loss: 0.0001, total_Loss: 0.2986, acc: 0.7627\n",
      "Epoch: [ 6/100], cls_loss: 0.2751, transfer_loss: 0.0001, total_Loss: 0.2760, acc: 0.5653\n",
      "Epoch: [ 7/100], cls_loss: 0.2329, transfer_loss: 0.0001, total_Loss: 0.2337, acc: 0.5414\n",
      "Epoch: [ 8/100], cls_loss: 0.2859, transfer_loss: 0.0001, total_Loss: 0.2866, acc: 0.5494\n",
      "Epoch: [ 9/100], cls_loss: 0.2575, transfer_loss: 0.0001, total_Loss: 0.2582, acc: 0.5414\n",
      "Epoch: [10/100], cls_loss: 0.2323, transfer_loss: 0.0001, total_Loss: 0.2331, acc: 0.6178\n",
      "Epoch: [11/100], cls_loss: 0.2677, transfer_loss: 0.0001, total_Loss: 0.2683, acc: 0.6959\n",
      "Epoch: [12/100], cls_loss: 0.2362, transfer_loss: 0.0000, total_Loss: 0.2367, acc: 0.6529\n",
      "Epoch: [13/100], cls_loss: 0.2332, transfer_loss: 0.0000, total_Loss: 0.2337, acc: 0.5573\n",
      "Epoch: [14/100], cls_loss: 0.2361, transfer_loss: 0.0000, total_Loss: 0.2365, acc: 0.5971\n",
      "Epoch: [15/100], cls_loss: 0.2557, transfer_loss: 0.0000, total_Loss: 0.2561, acc: 0.5876\n",
      "Epoch: [16/100], cls_loss: 0.1930, transfer_loss: 0.0000, total_Loss: 0.1933, acc: 0.5669\n",
      "Epoch: [17/100], cls_loss: 0.2124, transfer_loss: 0.0000, total_Loss: 0.2127, acc: 0.5796\n",
      "Epoch: [18/100], cls_loss: 0.2175, transfer_loss: 0.0000, total_Loss: 0.2179, acc: 0.5048\n",
      "Epoch: [19/100], cls_loss: 0.1836, transfer_loss: 0.0000, total_Loss: 0.1839, acc: 0.5541\n",
      "Epoch: [20/100], cls_loss: 0.3306, transfer_loss: 0.0000, total_Loss: 0.3309, acc: 0.5844\n",
      "Epoch: [21/100], cls_loss: 0.2510, transfer_loss: 0.0000, total_Loss: 0.2513, acc: 0.5446\n",
      "Epoch: [22/100], cls_loss: 0.2500, transfer_loss: 0.0000, total_Loss: 0.2503, acc: 0.5685\n",
      "Epoch: [23/100], cls_loss: 0.1813, transfer_loss: 0.0000, total_Loss: 0.1815, acc: 0.5191\n",
      "Epoch: [24/100], cls_loss: 0.1824, transfer_loss: 0.0000, total_Loss: 0.1826, acc: 0.5318\n",
      "Epoch: [25/100], cls_loss: 0.2472, transfer_loss: 0.0000, total_Loss: 0.2474, acc: 0.5812\n"
     ]
    }
   ],
   "source": [
    "train(dataloaders, transfer_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yWvYODRDtm43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4119384/827170216.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7627388535031847\n"
     ]
    }
   ],
   "source": [
    "transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n",
    "acc_test = test(transfer_model, dataloaders['tar'])\n",
    "print(f'Test accuracy: {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbw6KndNtm43"
   },
   "source": [
    "Now we are done."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
